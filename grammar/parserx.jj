options
{
    JAVA_UNICODE_ESCAPE = true;
    //LOOKAHEAD=3;
    STATIC=false;
}

PARSER_BEGIN(GParser)

package grammar;

import nodes.*;
import rule.*;
import utils.*;
import java.util.*;
import java.io.File;

public class GParser
{
  String getStringValue(String str){
    if(str.startsWith("\"")&&str.endsWith("\"")){
      return str.substring(1,str.length()-1);
    }
    return str;
  }
}
PARSER_END(GParser)

SKIP :
{
    " "  |   "\r"  |  "\n"  |  "\t"
}

MORE :
{
  "//" : IN_SINGLE_LINE_COMMENT
|
  <"/**" ~["/"]> { input_stream.backup(1); } : IN_FORMAL_COMMENT
|
  "/*" : IN_MULTI_LINE_COMMENT
}

<IN_SINGLE_LINE_COMMENT>
SPECIAL_TOKEN :
{
  <SINGLE_LINE_COMMENT: "\n" | "\r" | "\r\n" > : DEFAULT
}

<IN_FORMAL_COMMENT>
SPECIAL_TOKEN :
{
  <FORMAL_COMMENT: "*/" > : DEFAULT
}

<IN_MULTI_LINE_COMMENT>
SPECIAL_TOKEN :
{
  <MULTI_LINE_COMMENT: "*/" > : DEFAULT
}

<IN_SINGLE_LINE_COMMENT,IN_FORMAL_COMMENT,IN_MULTI_LINE_COMMENT>
MORE :
{
  < ~[] >
}

TOKEN :
{
  < LPAREN: "(" >
| < RPAREN: ")" >
| < LBRACE: "{" >
| < RBRACE: "}" >
//| < LBRACKET: "[" >
//| < RBRACKET: "]" >
| < SEMI: ";" >
| < COLON: ":" >
| < DOT: ".">
| < COMMA: "," >
| < QUOTE: "'" >
| < EQ: "=">
| < STAR: "*">
| < PLUS: "+">
| < QUES: "?">
| < XOR: "^">//negation
| < TILDE: "~">//read until
| < HASH: "#">
}

TOKEN:
{
  <TOKEN_: "token">
| <TOKENS: "tokens">
| <SKIP_: "skip">
| <STRING_LITERAL: "\"" (~["\r","\n","\""] | "\\\"")* "\"">
| <INCLUDE: "include">
}

TOKEN:
{
  <IDENT: <CHAR> ( <CHAR> | ["0"-"9"] | "_" )* >
| <#CHAR: ["a"-"z","A"-"Z"]>
//| <#ESCAPED: ("\\" ["r","n","t","s"])>
//| <BRACKET_LIST: "[" ("^")? (<ESCAPED> | (<CHAR>))+ "]">
| <BRACKET_LIST: "["  (~["]"])* "]">
}

Tree tree(File file):
{
  Tree tree=new Tree();
  tree.file=file;
}
{
  (includeStatement(tree))* (tokenBlock(tree) | skipBlock(tree))*  (ruleDecl(tree))*
  {
    return tree;
  }
}

void includeStatement(Tree tree):
{
  Token tok;
}
{
  "include" tok=<STRING_LITERAL>
  {
    tree.addInclude(getStringValue(tok.image));
  }
}

//lexer rules

void tokenBlock(Tree tree):
{
}
{
 "tokens" "{" tokenList(tree,false) "}"
}

void tokenList(Tree tree,boolean skip):
{}
{
 (tokenDecl(tree,skip))*
}

void tokenDecl(Tree tree,boolean skip):
{
  String name;
  boolean frag=false;
  TokenDecl decl;
  Node rhs;
}
{
  ("#"{frag=true;})? name=name() "=" rhs=lexer_rhs()
  {
    decl=new TokenDecl(name);
    decl.fragment=frag;
    decl.regex=rhs;
    if(skip){tree.addSkip(decl);}
    else{tree.addToken(decl);}
  }
}

void skipBlock(Tree tree):
{
}
{
 "skip" "{" tokenList(tree,true) "}"
}

Node lexer_rhs():
{
  Node rule;
  OrNode or=new OrNode();
  boolean more=false;
}
{
  rule=lexer_rhs_list(){or.add(rule);}
   ("|" rule=lexer_rhs_list()
    {
      or.add(rule);
      more=true;
    })*
   { return more?or:rule; }
}

Node lexer_rhs_list():
{
  Sequence s=new Sequence();
  Node r;
}
{
  (r=lexer_regex(){s.add(r);})+
  {return s.normal();}
}

Node lexer_regex():
{
  Node rule;
  RegexNode regex;
}
{
  rule=lexer_simple(){regex=new RegexNode(rule);}
   (("*"{regex.star=true;}
  | "+"{regex.plus=true;}
  | "?"{regex.optional=true;}){return regex;})?
     {return rule;}
}

Node lexer_simple():
{
  Node rule;
}
{
  (rule=lexer_group()
   | rule=lexerRef()//before nameRule
   | rule=stringNode()
   | rule=bracketNode()
   | rule=dotNode()){return rule;}
}

Node dotNode():
{}
{
  <DOT>{
    StringNode str=new StringNode();
    str.isDot=true;
    return str;
  }
}
//[a-z]
Node bracketNode():
{
  Bracket b=new Bracket();
  Token t;
}
{
  t=<BRACKET_LIST>
  {
    b.parse(t.image);
    return b;
  }
}

Node lexerRef():
{
  String s;
  NameNode ref;
}
{
 "{" s=name() "}"
 {
   ref=new NameNode(s);
   ref.isToken=true;
   return ref;
 }
}

Node lexer_group():
{
  GroupNode group=new GroupNode();
  Node rule;
}
{
  "(" rule=lexer_rhs() ")"
  {
    group.rhs=rule;
    return group;
  }
}

//--------------------------------
//parser rules

void ruleDecl(Tree tree):
{
  RuleDecl decl=new RuleDecl();
  String name;
  Node rhs;
}
{
  name=name() ("=" | ":") rhs=rhs()  ";"
  {
    decl.name=name;
    decl.rhs=rhs;
    tree.addRule(decl);
  }
}

//or series
Node rhs():
{
  Node rule;
  OrNode or=new OrNode();
  boolean more=false;
}
{
  rule=rhs_list(){or.add(rule);}
   ("|" rule=rhs_list()
    {
      or.add(rule);
      more=true;
    })*
   { return more?or:rule; }
}

//sequence no or
Node rhs_list():
{
  Sequence s=new Sequence();
  Node r;
}
{
  (r=regex(){s.add(r);})+
  {return s.normal();}
}

Node regex():
{
  Node rule;
  RegexNode regex;
}
{
  rule=simple(){regex=new RegexNode(rule);}
   (("*"{regex.star=true;}
  | "+"{regex.plus=true;}
  | "?"{regex.optional=true;}){return regex;})?
     {return rule;}
}

Node simple():
{
  Node rule;
}
{
  (rule=groupRule()
   | rule=nameRule()
   | rule=stringNode())
   {return rule;}
}

Node stringNode():
{
  Token tok;
}
{
  tok=<STRING_LITERAL>{
    String str = Util.fromEscaped(Util.trim(tok.image));
    return new StringNode(str);
  }
}

Node groupRule():
{
  GroupNode group=new GroupNode();
  Node rule;
}
{
  "(" rule=rhs() ")"
  {
    group.rhs=rule;
    return group;
  }
}


Node nameRule():
{
  String name;
}
{
  name = name(){
    return new NameNode(name);
  }
}

String name():
{
  Token token;
}
{
  token=<IDENT>{return token.image;}
}
